{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e5e37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RSNA Intracranial Aneurysm Detection — GBM 2-stage (rápido)\n",
    "# LGBM + XGB + CatBoost + ExtraTrees | Sin internet | Kaggle API OK\n",
    "# ============================================================\n",
    "\n",
    "# ---- Imports\n",
    "import os, json, random, warnings, gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "import pydicom\n",
    "import joblib\n",
    "\n",
    "import kaggle_evaluation.rsna_inference_server\n",
    "\n",
    "# ============================================================\n",
    "# Config (FAST)\n",
    "# ============================================================\n",
    "INPUT_DIR  = \"/kaggle/input/rsna-intracranial-aneurysm-detection\"\n",
    "SERIES_DIR = os.path.join(INPUT_DIR, \"series\")\n",
    "TRAIN_CSV  = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---- Ajustes de velocidad/calidad\n",
    "N_FOLDS        = 2          # 2 = rápido; si cabe tiempo usa 3\n",
    "READ_INTENS    = True       # estadísticos de intensidades submuestreadas\n",
    "FAST_SLICES    = 24         # nº máx de cortes por serie (24/32/48)\n",
    "STRIDE_INTENS  = 32         # stride sobre píxeles para intensidades\n",
    "\n",
    "MAX_ROUNDS_LGB = 1200       # rondas base\n",
    "MAX_ROUNDS_XGB = 1200\n",
    "CAT_ITERS      = 6000\n",
    "\n",
    "LGB_NUM_THREADS = 8\n",
    "XGB_NTHREAD     = 8\n",
    "CAT_THREAD_COUNT= 8\n",
    "ET_N_JOBS = 4\n",
    "\n",
    "FEAT_CACHE = \"/kaggle/working/features_fast.parquet\"  # cache de features\n",
    "\n",
    "# extra config for meta stacking\n",
    "META_FOLDS = 3\n",
    "\n",
    "# Etiquetas\n",
    "LABELS_13 = [\n",
    " 'Left Infraclinoid Internal Carotid Artery',\n",
    " 'Right Infraclinoid Internal Carotid Artery',\n",
    " 'Left Supraclinoid Internal Carotid Artery',\n",
    " 'Right Supraclinoid Internal Carotid Artery',\n",
    " 'Left Middle Cerebral Artery',\n",
    " 'Right Middle Cerebral Artery',\n",
    " 'Anterior Communicating Artery',\n",
    " 'Left Anterior Cerebral Artery',\n",
    " 'Right Anterior Cerebral Artery',\n",
    " 'Left Posterior Communicating Artery',\n",
    " 'Right Posterior Communicating Artery',\n",
    " 'Basilar Tip',\n",
    " 'Other Posterior Circulation'\n",
    "]\n",
    "MAIN        = 'Aneurysm Present'\n",
    "ALL_TARGETS = LABELS_13 + [MAIN]\n",
    "\n",
    "# ============================================================\n",
    "# Utils\n",
    "# ============================================================\n",
    "def robust_stats(arr: np.ndarray):\n",
    "    if arr.size == 0:\n",
    "        return dict(mean=0, std=0, p1=0, p5=0, p50=0, p95=0, p99=0)\n",
    "    v = arr.astype(np.float32)\n",
    "    v = v[np.isfinite(v)]\n",
    "    if v.size == 0:\n",
    "        return dict(mean=0, std=0, p1=0, p5=0, p50=0, p95=0, p99=0)\n",
    "    p1, p99 = np.percentile(v, [1, 99])\n",
    "    v = np.clip(v, p1, p99)\n",
    "    return dict(\n",
    "        mean=float(v.mean()),\n",
    "        std=float(v.std() + 1e-6),\n",
    "        p1=float(np.percentile(v, 1)),\n",
    "        p5=float(np.percentile(v, 5)),\n",
    "        p50=float(np.percentile(v, 50)),\n",
    "        p95=float(np.percentile(v, 95)),\n",
    "        p99=float(np.percentile(v, 99)),\n",
    "    )\n",
    "\n",
    "# windows and histogram settings\n",
    "HU_WINDOWS = [(-100, 700), (-200, 200), (500, 1500)]\n",
    "HIST_BINS = 16\n",
    "\n",
    "def extract_series_features(series_path: str, max_slices: int = FAST_SLICES):\n",
    "    \"\"\"Lee metadatos y (opcional) intensidades submuestreadas para rapidez.\n",
    "    Añade: HU window stats, histograms, local-variance texture (3x3) features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = [os.path.join(series_path,f) for f in os.listdir(series_path) if f.endswith(\".dcm\")]\n",
    "    except Exception:\n",
    "        files = []\n",
    "    if not files:\n",
    "        # keep same keys as before\n",
    "        out = {\n",
    "            'NumSlices':0,'Rows_med':0,'Cols_med':0,\n",
    "            'PixelSpacingX_med':0.0,'PixelSpacingY_med':0.0,\n",
    "            'SliceThickness_med':0.0,\n",
    "            'Modality_tag':'UNK','is_CT_like':0,'is_MR_like':0\n",
    "        }\n",
    "        # add placeholders for intens stats\n",
    "        for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "            out[f'Int_{k}'] = 0.0\n",
    "        # hist bins\n",
    "        for i in range(HIST_BINS): out[f'Hist_ct_bin_{i}'] = 0.0\n",
    "        # texture placeholders\n",
    "        for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "            out[f'Tex3_{k}'] = 0.0\n",
    "        # HU window placeholders\n",
    "        for wi in range(len(HU_WINDOWS)):\n",
    "            for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "                out[f'HU_w{wi}_{k}'] = 0.0\n",
    "        return out\n",
    "\n",
    "    try:\n",
    "        files = sorted(files, key=lambda p: getattr(pydicom.dcmread(p, stop_before_pixels=True, force=True), 'InstanceNumber', 0))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    n = len(files)\n",
    "    chosen = files if n <= max_slices else [files[i] for i in np.linspace(0, n-1, max_slices).astype(int)]\n",
    "\n",
    "    rows, cols, px, py, th = [], [], [], [], []\n",
    "    modality_tag = None\n",
    "    intens_list = []\n",
    "    texture_vals = []\n",
    "    step_slice = max(1, len(chosen)//max(1, min(len(chosen), 32)))\n",
    "\n",
    "    for i, fp in enumerate(chosen):\n",
    "        try:\n",
    "            ds_meta = pydicom.dcmread(fp, stop_before_pixels=True, force=True)\n",
    "            if modality_tag is None:\n",
    "                modality_tag = getattr(ds_meta, 'Modality', None)\n",
    "            if hasattr(ds_meta, 'Rows'): rows.append(int(ds_meta.Rows))\n",
    "            if hasattr(ds_meta, 'Columns'): cols.append(int(ds_meta.Columns))\n",
    "            if hasattr(ds_meta, 'PixelSpacing') and len(ds_meta.PixelSpacing)==2:\n",
    "                px.append(float(ds_meta.PixelSpacing[0])); py.append(float(ds_meta.PixelSpacing[1]))\n",
    "            if hasattr(ds_meta, 'SliceThickness'):\n",
    "                th.append(float(ds_meta.SliceThickness))\n",
    "\n",
    "            if READ_INTENS and i % step_slice == 0:\n",
    "                ds = pydicom.dcmread(fp, force=True)\n",
    "                if hasattr(ds, 'PixelData'):\n",
    "                    arr = ds.pixel_array.astype(np.float32)\n",
    "                    # apply standard CT window for intensity stats by default\n",
    "                    if modality_tag is not None and str(modality_tag).upper().startswith(\"CT\"):\n",
    "                        slope = float(getattr(ds, 'RescaleSlope', 1.0))\n",
    "                        intercept = float(getattr(ds, 'RescaleIntercept', 0.0))\n",
    "                        arr = arr * slope + intercept\n",
    "                    # collect intensities (sampled)\n",
    "                    try:\n",
    "                        intens_list.append(arr.ravel()[::STRIDE_INTENS])\n",
    "                    except Exception:\n",
    "                        intens_list.append(arr.ravel())\n",
    "\n",
    "                    # local 3x3 variance texture (fast using uniform_filter)\n",
    "                    try:\n",
    "                        mean_local = ndi.uniform_filter(arr, size=3)\n",
    "                        mean_sq_local = ndi.uniform_filter(arr*arr, size=3)\n",
    "                        local_var = mean_sq_local - mean_local*mean_local\n",
    "                        texture_vals.append(local_var.ravel()[::STRIDE_INTENS])\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    feats = dict(\n",
    "        NumSlices=n,\n",
    "        Rows_med=int(np.median(rows)) if rows else 0,\n",
    "        Cols_med=int(np.median(cols)) if cols else 0,\n",
    "        PixelSpacingX_med=float(np.median(px)) if px else 0.0,\n",
    "        PixelSpacingY_med=float(np.median(py)) if py else 0.0,\n",
    "        SliceThickness_med=float(np.median(th)) if th else 0.0,\n",
    "    )\n",
    "\n",
    "    # combined intens array for histogram/windows/stats\n",
    "    if READ_INTENS and intens_list:\n",
    "        try:\n",
    "            all_int = np.concatenate(intens_list)\n",
    "        except Exception:\n",
    "            all_int = intens_list[0] if intens_list else np.array([])\n",
    "        st = robust_stats(all_int)\n",
    "        for k, v in st.items(): feats[f'Int_{k}'] = v\n",
    "\n",
    "        # HU windows stats\n",
    "        for wi, (low, high) in enumerate(HU_WINDOWS):\n",
    "            arr_win = np.clip(all_int, low, high)\n",
    "            stw = robust_stats(arr_win)\n",
    "            for k, v in stw.items(): feats[f'HU_w{wi}_{k}'] = v\n",
    "\n",
    "        # histogram over CT window (first window assumed CT-like)\n",
    "        try:\n",
    "            low, high = HU_WINDOWS[0]\n",
    "            hist_vals, _ = np.histogram(np.clip(all_int, low, high), bins=HIST_BINS, range=(low, high), density=True)\n",
    "            for i, h in enumerate(hist_vals): feats[f'Hist_ct_bin_{i}'] = float(h)\n",
    "        except Exception:\n",
    "            for i in range(HIST_BINS): feats[f'Hist_ct_bin_{i}'] = 0.0\n",
    "\n",
    "        # texture stats (concat across sampled slices)\n",
    "        if texture_vals:\n",
    "            try:\n",
    "                all_tex = np.concatenate(texture_vals)\n",
    "                stt = robust_stats(all_tex)\n",
    "                for k, v in stt.items(): feats[f'Tex3_{k}'] = v\n",
    "            except Exception:\n",
    "                for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "                    feats[f'Tex3_{k}'] = 0.0\n",
    "        else:\n",
    "            for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "                feats[f'Tex3_{k}'] = 0.0\n",
    "\n",
    "    else:\n",
    "        # defaults if no intensity\n",
    "        for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "            feats[f'Int_{k}'] = 0.0\n",
    "        for wi in range(len(HU_WINDOWS)):\n",
    "            for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "                feats[f'HU_w{wi}_{k}'] = 0.0\n",
    "        for i in range(HIST_BINS): feats[f'Hist_ct_bin_{i}'] = 0.0\n",
    "        for k in ['mean','std','p1','p5','p50','p95','p99']:\n",
    "            feats[f'Tex3_{k}'] = 0.0\n",
    "\n",
    "    tag = str(modality_tag) if modality_tag is not None else 'UNK'\n",
    "    feats['Modality_tag'] = tag\n",
    "    feats['is_CT_like'] = 1 if 'CT' in tag.upper() else 0\n",
    "    feats['is_MR_like'] = 1 if 'MR' in tag.upper() else 0\n",
    "    return feats\n",
    "\n",
    "def mean_weighted_columnwise_auc(y_true_df, y_pred_df):\n",
    "    y, p = y_true_df[MAIN].values, y_pred_df[MAIN].values\n",
    "    main_auc = roc_auc_score(y, p) if len(np.unique(y))>1 else np.nan\n",
    "    loc_aucs = []\n",
    "    for c in LABELS_13:\n",
    "        yy, pp = y_true_df[c].values, y_pred_df[c].values\n",
    "        if len(np.unique(yy))>1: loc_aucs.append(roc_auc_score(yy, pp))\n",
    "    loc_mean = float(np.mean(loc_aucs)) if len(loc_aucs) else np.nan\n",
    "    return (main_auc + loc_mean)/2.0, main_auc, loc_mean\n",
    "\n",
    "# ============================================================\n",
    "# Carga y features (con caché)\n",
    "# ============================================================\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train['PatientSex'] = train['PatientSex'].astype(str)\n",
    "train['Modality']   = train['Modality'].astype(str)\n",
    "for c in ALL_TARGETS: train[c] = train[c].astype(float)\n",
    "\n",
    "series_ids = train['SeriesInstanceUID'].unique().tolist()\n",
    "\n",
    "if os.path.exists(FEAT_CACHE):\n",
    "    print(\"Leyendo features de cache:\", FEAT_CACHE)\n",
    "    feat_df = pd.read_parquet(FEAT_CACHE)\n",
    "else:\n",
    "    print(f\"Construyendo features para {len(series_ids)} series...\")\n",
    "    rows = []\n",
    "    for i, sid in enumerate(series_ids):\n",
    "        feats = extract_series_features(os.path.join(SERIES_DIR, sid), max_slices=FAST_SLICES)\n",
    "        feats['SeriesInstanceUID'] = sid\n",
    "        rows.append(feats)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[features] {i+1}/{len(series_ids)}\")\n",
    "    feat_df = pd.DataFrame(rows)\n",
    "    feat_df.to_parquet(FEAT_CACHE)\n",
    "    print(\"Cache de features guardada en\", FEAT_CACHE)\n",
    "\n",
    "df = train.merge(feat_df, on='SeriesInstanceUID', how='inner')\n",
    "\n",
    "# Codifica mínimas (no se usan como features)\n",
    "for col in ['PatientSex','Modality','Modality_tag']:\n",
    "    le = LabelEncoder(); df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# solo columnas numéricas válidas (fuera IDs/targets/categóricas)\n",
    "exclude = set(['SeriesInstanceUID','PatientSex','Modality','Modality_tag'] + ALL_TARGETS)\n",
    "feats = [c for c in df.columns if c not in exclude and np.issubdtype(df[c].dtype, np.number)]\n",
    "print(\"Nº de features:\", len(feats))\n",
    "\n",
    "# ============================================================\n",
    "# Folds y params\n",
    "# ============================================================\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "df['fold'] = -1\n",
    "for i, (_, va_idx) in enumerate(skf.split(df, df[MAIN].values)):\n",
    "    df.loc[va_idx, 'fold'] = i\n",
    "df['fold'] = df['fold'].astype(int)\n",
    "\n",
    "oof_bin = np.zeros(len(df))\n",
    "oof_loc = {lab: np.zeros(len(df)) for lab in LABELS_13}\n",
    "\n",
    "# guardar modelos por fold\n",
    "bin_models = {'lgb':[], 'xgb':[], 'cat':[], 'et':[]}\n",
    "loc_models = {lab:{'lgb':[], 'xgb':[], 'cat':[]} for lab in LABELS_13}\n",
    "\n",
    "pos_ratio = float((df[MAIN]==1).mean())\n",
    "scale_pos_weight = (1 - pos_ratio) / max(pos_ratio, 1e-6)\n",
    "\n",
    "lgb_params = dict(\n",
    "    objective='binary', metric='auc', learning_rate=0.03,\n",
    "    num_leaves=63, feature_fraction=0.8, bagging_fraction=0.8, bagging_freq=5,\n",
    "    lambda_l2=2.0, verbose=-1, seed=SEED, scale_pos_weight=scale_pos_weight,\n",
    "    num_threads=LGB_NUM_THREADS,\n",
    ")\n",
    "xgb_params_bin = dict(\n",
    "    objective='binary:logistic', eval_metric='auc', tree_method='hist',\n",
    "    learning_rate=0.03, max_depth=6, min_child_weight=8, subsample=0.8, colsample_bytree=0.8,\n",
    "    reg_lambda=2.0, seed=SEED, scale_pos_weight=float(scale_pos_weight),\n",
    "    nthread=XGB_NTHREAD,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Entrenamiento\n",
    "# ============================================================\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f\"\\n========== Fold {fold} ==========\")\n",
    "    tr = df[df['fold']!=fold]; va = df[df['fold']==fold]\n",
    "    X_tr, y_tr = tr[feats], tr[MAIN].values\n",
    "    X_va, y_va = va[feats], va[MAIN].values\n",
    "\n",
    "    # LGBM binario\n",
    "    dtr = lgb.Dataset(X_tr, label=y_tr); dva = lgb.Dataset(X_va, label=y_va)\n",
    "    lgb_bin = lgb.train(\n",
    "        params=lgb_params, train_set=dtr, num_boost_round=MAX_ROUNDS_LGB,\n",
    "        valid_sets=[dtr, dva], valid_names=['tr','va'],\n",
    "        callbacks=[lgb.early_stopping(200, verbose=False), lgb.log_evaluation(200)]\n",
    "    )\n",
    "    p_lgb = lgb_bin.predict(X_va, num_iteration=lgb_bin.best_iteration or lgb_bin.current_iteration())\n",
    "\n",
    "    # XGB binario\n",
    "    xg_tr, xg_va = xgb.DMatrix(X_tr, label=y_tr), xgb.DMatrix(X_va, label=y_va)\n",
    "    xgb_bin = xgb.train(\n",
    "        params=xgb_params_bin, dtrain=xg_tr, num_boost_round=MAX_ROUNDS_XGB,\n",
    "        evals=[(xg_tr,'tr'),(xg_va,'va')], early_stopping_rounds=200, verbose_eval=False\n",
    "    )\n",
    "    p_xgb = xgb_bin.predict(xg_va)\n",
    "\n",
    "    # CatBoost binario\n",
    "    cat_bin = CatBoostClassifier(\n",
    "        loss_function='Logloss', eval_metric='AUC', depth=6, learning_rate=0.03, l2_leaf_reg=6,\n",
    "        iterations=CAT_ITERS, od_type='Iter', od_wait=200, random_seed=SEED,\n",
    "        class_weights=[1.0, float(scale_pos_weight)], thread_count=CAT_THREAD_COUNT, verbose=False\n",
    "    )\n",
    "    cat_bin.fit(X_tr, y_tr, eval_set=(X_va, y_va), verbose=False)\n",
    "    p_cat = cat_bin.predict_proba(X_va)[:,1]\n",
    "\n",
    "    # ExtraTrees (fast tabular model)\n",
    "    et_clf = ExtraTreesClassifier(n_estimators=100, n_jobs=ET_N_JOBS, random_state=SEED)\n",
    "    et_clf.fit(X_tr, y_tr)\n",
    "    p_et = et_clf.predict_proba(X_va)[:,1]\n",
    "\n",
    "    # ensemble binario (keep per-model OOF for stacking)\n",
    "    p_bin_lgb = p_lgb\n",
    "    p_bin_xgb = p_xgb\n",
    "    p_bin_cat = p_cat\n",
    "    p_bin_et = p_et\n",
    "    p_bin = (p_bin_lgb + p_bin_xgb + p_bin_cat + p_bin_et)/4.0\n",
    "    oof_bin[va.index] = p_bin\n",
    "\n",
    "    # store models and oof per-model preds for stacking\n",
    "    bin_models['lgb'].append(lgb_bin)\n",
    "    bin_models['xgb'].append(xgb_bin)\n",
    "    bin_models['cat'].append(cat_bin)\n",
    "    bin_models['et'].append(et_clf)\n",
    "    # collect OOF columns for stacking\n",
    "    if fold == 0:\n",
    "        oof_stack_lgb = np.zeros(len(df))\n",
    "        oof_stack_xgb = np.zeros(len(df))\n",
    "        oof_stack_cat = np.zeros(len(df))\n",
    "        oof_stack_et = np.zeros(len(df))\n",
    "    oof_stack_lgb[va.index] = p_bin_lgb\n",
    "    oof_stack_xgb[va.index] = p_bin_xgb\n",
    "    oof_stack_cat[va.index] = p_bin_cat\n",
    "    oof_stack_et[va.index] = p_bin_et\n",
    "\n",
    "    # Etapa 2: OvR por etiqueta condicionada a positivos\n",
    "    tr_pos = tr[tr[MAIN]==1]\n",
    "    if len(tr_pos)==0: \n",
    "        print(\"Sin positivos en fold; salto localización.\")\n",
    "        continue\n",
    "    X_tr_pos = tr_pos[feats]\n",
    "\n",
    "    for lab in LABELS_13:\n",
    "        y_tr_loc = tr_pos[lab].values\n",
    "\n",
    "        # LGB\n",
    "        dtr_pos = lgb.Dataset(X_tr_pos, label=y_tr_loc)\n",
    "        lgb_loc = lgb.train(\n",
    "            params=dict(lgb_params, objective='binary'),\n",
    "            train_set=dtr_pos, num_boost_round=max(600, MAX_ROUNDS_LGB//2),\n",
    "            valid_sets=[dtr_pos], valid_names=['tr'],\n",
    "            callbacks=[lgb.early_stopping(120, verbose=False), lgb.log_evaluation(200)]\n",
    "        )\n",
    "        pl_lgb = lgb_loc.predict(X_va, num_iteration=lgb_loc.best_iteration or lgb_loc.current_iteration())\n",
    "\n",
    "        # XGB\n",
    "        xg_pos = xgb.DMatrix(X_tr_pos, label=y_tr_loc)\n",
    "        xgb_loc = xgb.train(\n",
    "            params=xgb_params_bin, dtrain=xg_pos, num_boost_round=max(600, MAX_ROUNDS_XGB//2),\n",
    "            evals=[(xg_pos,'tr')], early_stopping_rounds=120, verbose_eval=False\n",
    "        )\n",
    "        pl_xgb = xgb_loc.predict(xgb.DMatrix(X_va))\n",
    "\n",
    "        # CatBoost\n",
    "        cat_loc = CatBoostClassifier(\n",
    "            loss_function='Logloss', eval_metric='AUC', depth=6, learning_rate=0.03, l2_leaf_reg=6,\n",
    "            iterations=max(3000, CAT_ITERS//3), od_type='Iter', od_wait=150,\n",
    "            random_seed=SEED, thread_count=CAT_THREAD_COUNT, verbose=False\n",
    "        )\n",
    "        cat_loc.fit(X_tr_pos, y_tr_loc, verbose=False)\n",
    "        pl_cat = cat_loc.predict_proba(X_va)[:,1]\n",
    "\n",
    "        # P(loc) = P(aneu) * P(loc | aneu)\n",
    "        p_loc = p_bin * ((pl_lgb + pl_xgb + pl_cat)/3.0)\n",
    "        oof_loc[lab][va.index] = p_loc\n",
    "\n",
    "        loc_models[lab]['lgb'].append(lgb_loc)\n",
    "        loc_models[lab]['xgb'].append(xgb_loc)\n",
    "        loc_models[lab]['cat'].append(cat_loc)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# Métrica OOF\n",
    "# ============================================================\n",
    "oof_df = pd.DataFrame({MAIN:oof_bin}, index=df.index)\n",
    "for lab in LABELS_13: oof_df[lab] = oof_loc[lab]\n",
    "true_df = df[ALL_TARGETS].copy()\n",
    "mw_auc, auc_main, auc_13mean = mean_weighted_columnwise_auc(true_df, oof_df)\n",
    "\n",
    "print(f\"\\nOOF AUC (Aneurysm Present): {auc_main:.5f}\")\n",
    "print(f\"OOF AUC (mean 13 locs)    : {auc_13mean:.5f}\")\n",
    "print(f\"OOF FINAL (MWC-AUC)       : {mw_auc:.5f}\")\n",
    "\n",
    "# ============================================================\n",
    "# Stacking meta-model: KFold stacking + LightGBM shallow meta\n",
    "# ============================================================\n",
    "try:\n",
    "    stack_X = np.vstack([oof_stack_lgb, oof_stack_xgb, oof_stack_cat, oof_stack_et]).T\n",
    "    stack_y = df[MAIN].values\n",
    "    # decide meta folds\n",
    "    meta_folds = META_FOLDS if META_FOLDS <= len(df) else 2\n",
    "    meta_folds = min(meta_folds, N_FOLDS if N_FOLDS>0 else 2)\n",
    "    skf_meta = StratifiedKFold(n_splits=meta_folds, shuffle=True, random_state=SEED)\n",
    "    meta_oof = np.zeros(len(df))\n",
    "    meta_models = []\n",
    "    lgb_meta_params = dict(\n",
    "        objective='binary', metric='auc', learning_rate=0.05,\n",
    "        num_leaves=8, max_depth=3, verbose=-1, seed=SEED, num_threads=1\n",
    "    )\n",
    "    for mi, (mt_idx, mv_idx) in enumerate(skf_meta.split(stack_X, stack_y)):\n",
    "        Xm_tr, ym_tr = stack_X[mt_idx], stack_y[mt_idx]\n",
    "        Xm_va, ym_va = stack_X[mv_idx], stack_y[mv_idx]\n",
    "        dtr_m = lgb.Dataset(Xm_tr, label=ym_tr); dva_m = lgb.Dataset(Xm_va, label=ym_va)\n",
    "        meta_m = lgb.train(\n",
    "            params=lgb_meta_params, train_set=dtr_m, num_boost_round=500,\n",
    "            valid_sets=[dtr_m, dva_m], valid_names=['tr','va'],\n",
    "            callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(50)]\n",
    "        )\n",
    "        pred_m = meta_m.predict(Xm_va, num_iteration=meta_m.best_iteration or meta_m.current_iteration())\n",
    "        meta_oof[mv_idx] = pred_m\n",
    "        meta_models.append(meta_m)\n",
    "\n",
    "    # diagnostic\n",
    "    meta_auc = roc_auc_score(stack_y, meta_oof)\n",
    "    simple_auc = roc_auc_score(stack_y, oof_bin)\n",
    "    print(f\"Meta OOF AUC: {meta_auc:.5f}; Simple average OOF AUC: {simple_auc:.5f}\")\n",
    "\n",
    "    # train final meta on full data (shallow LGB) and save\n",
    "    dfull = lgb.Dataset(stack_X, label=stack_y)\n",
    "    final_meta = lgb.train(params=lgb_meta_params, train_set=dfull, num_boost_round=int(np.mean([m.best_iteration or 100 for m in meta_models]) if meta_models else 100))\n",
    "    os.makedirs(\"/kaggle/working/models_meta\", exist_ok=True)\n",
    "    final_meta.save_model(\"/kaggle/working/models_meta/meta_lgb_final.txt\")\n",
    "    # also keep a sklearn fallback (logreg) trained on full stack\n",
    "    logreg_fb = LogisticRegression(C=1.0, solver='lbfgs', max_iter=400)\n",
    "    logreg_fb.fit(stack_X, stack_y)\n",
    "    joblib.dump(logreg_fb, \"/kaggle/working/models_meta/logreg_meta.pkl\")\n",
    "except Exception as e:\n",
    "    print(\"Meta stacking failed:\", e)\n",
    "\n",
    "# ============================================================\n",
    "# Guardado modelos + metadatos para inferencia\n",
    "# ============================================================\n",
    "os.makedirs(\"/kaggle/working/models_bin\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/models_loc\", exist_ok=True)\n",
    "\n",
    "for i in range(len(bin_models['lgb'])):\n",
    "    joblib.dump(bin_models['lgb'][i], f\"/kaggle/working/models_bin/lgb_{i}.pkl\")\n",
    "    bin_models['xgb'][i].save_model(f\"/kaggle/working/models_bin/xgb_{i}.json\")\n",
    "    bin_models['cat'][i].save_model(f\"/kaggle/working/models_bin/cat_{i}.cbm\")\n",
    "    joblib.dump(bin_models['et'][i], f\"/kaggle/working/models_bin/et_{i}.pkl\")\n",
    "\n",
    "for lab in LABELS_13:\n",
    "    for i in range(len(loc_models[lab]['lgb'])):\n",
    "        joblib.dump(loc_models[lab]['lgb'][i], f\"/kaggle/working/models_loc/{lab}_lgb_{i}.pkl\")\n",
    "        loc_models[lab]['xgb'][i].save_model(f\"/kaggle/working/models_loc/{lab}_xgb_{i}.json\")\n",
    "        loc_models[lab]['cat'][i].save_model(f\"/kaggle/working/models_loc/{lab}_cat_{i}.cbm\")\n",
    "\n",
    "with open(\"/kaggle/working/feats_meta.json\",\"w\") as f:\n",
    "    json.dump({'feats':feats,'labels_13':LABELS_13,'main':MAIN}, f)\n",
    "\n",
    "print(\"Modelos y metadatos guardados.\")\n",
    "\n",
    "# ============================================================\n",
    "# Carga liviana para inferencia\n",
    "# ============================================================\n",
    "def _load_models_for_infer():\n",
    "    bin_loaded = {'lgb':[], 'xgb':[], 'cat':[], 'et':[]}\n",
    "    i = 0\n",
    "    while True:\n",
    "        p1 = f\"/kaggle/working/models_bin/lgb_{i}.pkl\"\n",
    "        p2 = f\"/kaggle/working/models_bin/xgb_{i}.json\"\n",
    "        p3 = f\"/kaggle/working/models_bin/cat_{i}.cbm\"\n",
    "        p4 = f\"/kaggle/working/models_bin/et_{i}.pkl\"\n",
    "        if not os.path.exists(p1): break\n",
    "        bin_loaded['lgb'].append(joblib.load(p1))\n",
    "        bx = xgb.Booster(); bx.load_model(p2); bin_loaded['xgb'].append(bx)\n",
    "        bc = CatBoostClassifier(); bc.load_model(p3); bin_loaded['cat'].append(bc)\n",
    "        if os.path.exists(p4):\n",
    "            bin_loaded['et'].append(joblib.load(p4))\n",
    "        else:\n",
    "            bin_loaded['et'].append(None)\n",
    "        i += 1\n",
    "\n",
    "    loc_loaded = {lab:{'lgb':[], 'xgb':[], 'cat':[]} for lab in LABELS_13}\n",
    "    for lab in LABELS_13:\n",
    "        i = 0\n",
    "        while True:\n",
    "            p1 = f\"/kaggle/working/models_loc/{lab}_lgb_{i}.pkl\"\n",
    "            p2 = f\"/kaggle/working/models_loc/{lab}_xgb_{i}.json\"\n",
    "            p3 = f\"/kaggle/working/models_loc/{lab}_cat_{i}.cbm\"\n",
    "            if not os.path.exists(p1): break\n",
    "            loc_loaded[lab]['lgb'].append(joblib.load(p1))\n",
    "            bx = xgb.Booster(); bx.load_model(p2); loc_loaded[lab]['xgb'].append(bx)\n",
    "            bc = CatBoostClassifier(); bc.load_model(p3); loc_loaded[lab]['cat'].append(bc)\n",
    "            i += 1\n",
    "\n",
    "    # try load meta-model (LightGBM final) if present, else sklearn fallback\n",
    "    meta_model = None\n",
    "    meta_path_lgb = \"/kaggle/working/models_meta/meta_lgb_final.txt\"\n",
    "    meta_path_sk = \"/kaggle/working/models_meta/logreg_meta.pkl\"\n",
    "    if os.path.exists(meta_path_lgb):\n",
    "        try:\n",
    "            meta_model = lgb.Booster(model_file=meta_path_lgb)\n",
    "        except Exception:\n",
    "            meta_model = None\n",
    "    elif os.path.exists(meta_path_sk):\n",
    "        try:\n",
    "            meta_model = joblib.load(meta_path_sk)\n",
    "        except Exception:\n",
    "            meta_model = None\n",
    "    return bin_loaded, loc_loaded, meta_model\n",
    "\n",
    "BIN_MDL, LOC_MDL, META_META = _load_models_for_infer()\n",
    "\n",
    "with open(\"/kaggle/working/feats_meta.json\") as f:\n",
    "    META = json.load(f)\n",
    "FEATS_LIST = META['feats']\n",
    "\n",
    "def predict_one_from_features(fnum: dict):\n",
    "    X = pd.DataFrame([fnum])[FEATS_LIST].astype(float)\n",
    "\n",
    "    # per-fold blended preds (as before)\n",
    "    pb_folds = []\n",
    "    # also keep per-model average across folds for stacking meta\n",
    "    pl_g = []\n",
    "    px_g = []\n",
    "    pc_g = []\n",
    "    pe_g = []\n",
    "    for i in range(len(BIN_MDL['lgb'])):\n",
    "        pb_l = BIN_MDL['lgb'][i].predict(X)\n",
    "        pb_x = BIN_MDL['xgb'][i].predict(xgb.DMatrix(X))\n",
    "        pb_c = BIN_MDL['cat'][i].predict_proba(X)[:,1]\n",
    "        pb_e = BIN_MDL['et'][i].predict_proba(X)[:,1] if (i < len(BIN_MDL['et']) and BIN_MDL['et'][i] is not None) else np.array([0.5])\n",
    "        pb_folds.append((pb_l + pb_x + pb_c + pb_e)/4.0)\n",
    "        pl_g.append(pb_l)\n",
    "        px_g.append(pb_x)\n",
    "        pc_g.append(pb_c)\n",
    "        pe_g.append(pb_e)\n",
    "    P_aneu_foldblend = float(np.mean(pb_folds)) if pb_folds else 0.5\n",
    "\n",
    "    # per-model average across folds for stacking input\n",
    "    pred_lgb_avg = float(np.mean(pl_g)) if pl_g else 0.5\n",
    "    pred_xgb_avg = float(np.mean(px_g)) if px_g else 0.5\n",
    "    pred_cat_avg = float(np.mean(pc_g)) if pc_g else 0.5\n",
    "    pred_et_avg = float(np.mean(pe_g)) if pe_g else 0.5\n",
    "\n",
    "    # meta prediction if available\n",
    "    P_meta = None\n",
    "    try:\n",
    "        if META_META is not None:\n",
    "            xin = np.array([[pred_lgb_avg, pred_xgb_avg, pred_cat_avg, pred_et_avg]])\n",
    "            # lgb.Booster has predict, sklearn has predict_proba\n",
    "            if isinstance(META_META, lgb.basic.Booster):\n",
    "                P_meta = float(META_META.predict(xin)[0])\n",
    "            else:\n",
    "                P_meta = float(META_META.predict_proba(xin)[:,1])\n",
    "    except Exception:\n",
    "        P_meta = None\n",
    "\n",
    "    # final aneurysm probability: use meta if available, else fallback to fold-blend\n",
    "    if P_meta is not None:\n",
    "        P_aneu = P_meta\n",
    "    else:\n",
    "        P_aneu = P_aneu_foldblend\n",
    "\n",
    "    loc_probs = {}\n",
    "    for lab in LABELS_13:\n",
    "        pl = []\n",
    "        for i in range(len(LOC_MDL[lab]['lgb'])):\n",
    "            pl_l = LOC_MDL[lab]['lgb'][i].predict(X)\n",
    "            pl_x = LOC_MDL[lab]['xgb'][i].predict(xgb.DMatrix(X))\n",
    "            pl_c = LOC_MDL[lab]['cat'][i].predict_proba(X)[:,1]\n",
    "            pl.append((pl_l + pl_x + pl_c)/3.0)\n",
    "        P_loc_cond = float(np.mean(pl)) if pl else 0.1\n",
    "        loc_probs[lab] = float(np.clip(P_aneu * P_loc_cond, 0, 1))\n",
    "\n",
    "    out = {MAIN: P_aneu}; out.update(loc_probs)\n",
    "    return out\n",
    "\n",
    "def predict(series_path: str) -> pd.DataFrame:\n",
    "    feats_dict = extract_series_features(series_path, max_slices=FAST_SLICES)\n",
    "    clean = {}\n",
    "    for c in FEATS_LIST:\n",
    "        val = feats_dict.get(c, 0.0)\n",
    "        try: clean[c] = float(val)\n",
    "        except: clean[c] = 0.0\n",
    "    probs = predict_one_from_features(clean)\n",
    "    return pd.DataFrame([probs])[ALL_TARGETS]\n",
    "\n",
    "# ============================================================\n",
    "# SERVIDOR DE EVALUACIÓN (OBLIGATORIO EN SUBMIT)\n",
    "# ============================================================\n",
    "inference_server = kaggle_evaluation.rsna_inference_server.RSNAInferenceServer(predict)\n",
    "\n",
    "is_submit = os.environ.get(\"KAGGLE_IS_COMPETITION_RERUN\", \"0\") == \"1\"\n",
    "\n",
    "if is_submit:\n",
    "    print(\"Kaggle submit run -> starting inference server\")\n",
    "    inference_server.serve()   # <- se queda a la espera mientras evalúan el test\n",
    "else:\n",
    "    print(\"Commit/interactive run -> creando preview de submission.parquet\")\n",
    "    inference_server.run_local_gateway()  # escribe /kaggle/working/submission.parquet\n",
    "    try:\n",
    "        display(pd.read_parquet('/kaggle/working/submission.parquet').head())\n",
    "    except Exception as e:\n",
    "        print(\"Preview no disponible:\", e)\n",
    "\n",
    "# CSV placeholder para que Kaggle deje pulsar Submit si estás en sesión interactiva\n",
    "pd.DataFrame({\"ok\":[1]}).to_csv(\"/kaggle/working/submission.csv\", index=False)\n",
    "print(\"submission.csv (placeholder) escrito.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
